{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install streamlit lasio pandas numpy scikit-learn xgboost matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMdgQr2XPG6K",
        "outputId": "d4e716d2-55de-4f29-fc99-3184dacf109f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting lasio\n",
            "  Downloading lasio-0.31-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lasio-0.31-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, lasio, pydeck, streamlit\n",
            "Successfully installed lasio-0.31 pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from xgboost import XGBClassifier # or XGBRegressor, depending on your task\n",
        "\n",
        "model = joblib.load('model.pkl')\n",
        "\n",
        "joblib.dump(model, 'model.pkl')"
      ],
      "metadata": {
        "id": "C3-VO5CaQlaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gtWSuejOeIx",
        "outputId": "f18a954c-2664-46e8-e64c-5f3fbe10edf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-07 12:39:04.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.146 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 12:39:04.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import lasio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from io import StringIO, BytesIO, TextIOWrapper\n",
        "\n",
        "# Define the features the model was trained on\n",
        "# Update this list to include all 34 features that your model expects\n",
        "# For now, we'll use the currently defined 8 features and add a mechanism to handle the mismatch\n",
        "CURRENT_FEATURES = ['GR', 'RHOB', 'RILD', 'DT', 'SP', 'SPOR', 'RILM', 'RLL3']\n",
        "LITHOLOGY_MAPPING = {0: 'Shale', 1: 'Dolomite', 2: 'Limestone', 3: 'Sandstone', 4: 'Siltstone'}\n",
        "\n",
        "# Load the pre-trained model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    try:\n",
        "        model = joblib.load('model.pkl')\n",
        "        # Get the actual number of features the model was trained on\n",
        "        if hasattr(model, 'n_features_in_'):\n",
        "            actual_features = model.n_features_in_\n",
        "        elif hasattr(model, 'feature_names_in_'):\n",
        "            actual_features = len(model.feature_names_in_)\n",
        "        elif hasattr(model, 'get_booster') and hasattr(model.get_booster(), 'num_features'):\n",
        "            actual_features = model.get_booster().num_features()\n",
        "        else:\n",
        "            # Default to the current features if can't determine\n",
        "            actual_features = len(CURRENT_FEATURES)\n",
        "        return model, actual_features\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Model file 'model.pkl' not found. Please ensure the model file is in the same directory as this script.\")\n",
        "        st.stop()\n",
        "\n",
        "# Streamlit app setup\n",
        "st.title(\"Kansas Basin Well Log Analyzer\")\n",
        "st.markdown(\"Upload an LAS file to preprocess, predict lithology labels, and visualize the results.\")\n",
        "\n",
        "# Load model and get actual feature count\n",
        "model, actual_feature_count = load_model()\n",
        "st.info(f\"Loaded model expects {actual_feature_count} features. Current configuration uses {len(CURRENT_FEATURES)} features.\")\n",
        "\n",
        "# File uploader for LAS file\n",
        "uploaded_file = st.file_uploader(\"Upload an LAS file\", type=['las'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Read the LAS file\n",
        "    st.subheader(\"Step 1: Reading the LAS File\")\n",
        "    try:\n",
        "        # Read the file content as bytes and wrap in TextIOWrapper for lasio\n",
        "        file_content = uploaded_file.read()\n",
        "        text_stream = TextIOWrapper(BytesIO(file_content), encoding='utf-8')\n",
        "        las = lasio.read(text_stream)\n",
        "        df = las.df().reset_index()  # Convert LAS to DataFrame\n",
        "        st.write(\"Original Data Preview:\")\n",
        "        st.dataframe(df.head())\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading LAS file: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Check if 'DEPTH' column exists and rename to 'DEPT'\n",
        "    if 'DEPTH' in df.columns:\n",
        "        st.write(\"Renaming 'DEPTH' column to 'DEPT' for consistency...\")\n",
        "        df.rename(columns={'DEPTH': 'DEPT'}, inplace=True)\n",
        "        # Update the LAS object to reflect the renamed column\n",
        "        for curve in las.curves:\n",
        "            if curve.mnemonic == 'DEPTH':\n",
        "                curve.mnemonic = 'DEPT'\n",
        "                break\n",
        "    elif 'DEPT' not in df.columns:\n",
        "        st.error(\"No 'DEPT' or 'DEPTH' column found in the LAS file. Please ensure the file contains a depth column.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Feature handling based on model requirements\n",
        "    st.subheader(\"Step 2: Feature Analysis\")\n",
        "\n",
        "    # Collect all available log curves from the LAS file\n",
        "    available_features = [col for col in df.columns if col != 'DEPT']\n",
        "    st.write(f\"Available features in LAS file: {', '.join(available_features)}\")\n",
        "\n",
        "    # Display feature mismatch warning if needed\n",
        "    if actual_feature_count != len(CURRENT_FEATURES):\n",
        "        st.warning(f\"\"\"\n",
        "        **Model/Feature Mismatch Detected**\n",
        "\n",
        "        Your model was trained on {actual_feature_count} features, but your code is configured for {len(CURRENT_FEATURES)} features.\n",
        "\n",
        "        To resolve this issue, you have two options:\n",
        "        1. Update the CURRENT_FEATURES list in your code to include all features used during training\n",
        "        2. Retrain your model using only the {len(CURRENT_FEATURES)} features currently defined\n",
        "\n",
        "        For now, we'll attempt to adapt the input data, but predictions may not be accurate.\n",
        "        \"\"\")\n",
        "\n",
        "    # Analysis mode selection\n",
        "    analysis_mode = st.radio(\n",
        "        \"Select analysis mode:\",\n",
        "        [\"Use available features (may be less accurate)\", \"View available features only (no prediction)\"]\n",
        "    )\n",
        "\n",
        "    if analysis_mode == \"View available features only (no prediction)\":\n",
        "        # Just display the available data\n",
        "        st.subheader(\"Available Well Log Data\")\n",
        "        st.dataframe(df)\n",
        "\n",
        "        # Plot available logs\n",
        "        st.subheader(\"Well Log Visualization\")\n",
        "        plot_features = st.multiselect(\"Select logs to display:\", available_features)\n",
        "\n",
        "        if plot_features:\n",
        "            fig, axes = plt.subplots(nrows=1, ncols=len(plot_features), figsize=(15, 10), sharey=True)\n",
        "\n",
        "            # Handle single feature case\n",
        "            if len(plot_features) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            # Plot each selected feature\n",
        "            for i, feature in enumerate(plot_features):\n",
        "                axes[i].plot(df[feature], df['DEPT'], label=feature)\n",
        "                axes[i].set_title(feature)\n",
        "                axes[i].invert_yaxis()\n",
        "                axes[i].grid(True)\n",
        "                if i == 0:\n",
        "                    axes[i].set_ylabel('Depth (ft)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "    else:\n",
        "        # Identify which current features are available in the dataset\n",
        "        available_current_features = [f for f in CURRENT_FEATURES if f in df.columns]\n",
        "        missing_features = [f for f in CURRENT_FEATURES if f not in df.columns]\n",
        "\n",
        "        # Add missing features as NaN columns\n",
        "        for feature in missing_features:\n",
        "            df[feature] = np.nan  # Add missing columns with NaN\n",
        "\n",
        "        if missing_features:\n",
        "            st.warning(f\"Features {missing_features} are missing and were added as NaN. Predictions may be less accurate.\")\n",
        "\n",
        "        # Remove rows where all features are NaN\n",
        "        st.subheader(\"Step 3: Filtering Rows with Missing Features\")\n",
        "        st.write(\"Removing rows where all features are NaN...\")\n",
        "        # Debug: Show the state of feature columns before filtering\n",
        "        st.write(\"Feature Columns (Before Filtering):\")\n",
        "        st.dataframe(df[CURRENT_FEATURES].head())\n",
        "\n",
        "        # Filter out rows where all features are NaN\n",
        "        df_filtered = df.dropna(subset=CURRENT_FEATURES, how='all')\n",
        "\n",
        "        # Debug: Show the filtered DataFrame\n",
        "        st.write(\"Data After Filtering (Rows with at least one non-NaN feature):\")\n",
        "        st.dataframe(df_filtered.head())\n",
        "\n",
        "        # Check if the filtered DataFrame is empty\n",
        "        if df_filtered.empty:\n",
        "            st.error(\"After filtering, no rows remain because all features are NaN in all rows. Please upload an LAS file with at least some non-NaN feature data.\")\n",
        "            st.stop()\n",
        "\n",
        "        # Update df to use the filtered DataFrame\n",
        "        df = df_filtered\n",
        "\n",
        "        # Preprocess the data\n",
        "        st.subheader(\"Step 4: Preprocessing the Data\")\n",
        "\n",
        "        # Feature engineering based on model requirements\n",
        "        if actual_feature_count > len(CURRENT_FEATURES):\n",
        "            st.warning(f\"\"\"\n",
        "            Model expects {actual_feature_count} features but we only have {len(CURRENT_FEATURES)} features defined.\n",
        "            We will attempt to adapt the input by adding synthetic features to match the model's expectations.\n",
        "            \"\"\")\n",
        "\n",
        "            # Strategy: Create derived features from existing ones to match the expected count\n",
        "            # This is a basic approach - ideally you'd know which specific features the model needs\n",
        "            X_prepared = df[CURRENT_FEATURES].copy()\n",
        "\n",
        "            # Fill NaNs with mean of each column\n",
        "            for col in X_prepared.columns:\n",
        "                X_prepared[col] = X_prepared[col].fillna(X_prepared[col].mean())\n",
        "                # Replace remaining NaNs (if a column is all NaN) with 0\n",
        "                X_prepared[col] = X_prepared[col].fillna(0)\n",
        "\n",
        "            # Scale features to [0, 1] range\n",
        "            scaler = MinMaxScaler()\n",
        "            X_scaled = scaler.fit_transform(X_prepared)\n",
        "            X_prepared = pd.DataFrame(X_scaled, columns=CURRENT_FEATURES)\n",
        "\n",
        "            # Add synthetic features to match the expected feature count\n",
        "            features_to_add = actual_feature_count - len(CURRENT_FEATURES)\n",
        "            synthetic_features = []\n",
        "\n",
        "            for i in range(features_to_add):\n",
        "                # Create synthetic features as combinations of existing ones\n",
        "                if i < len(CURRENT_FEATURES):\n",
        "                    # Use simple transforms of existing features\n",
        "                    feature_name = f\"SYNTH_{i+1}\"\n",
        "                    X_prepared[feature_name] = X_prepared[CURRENT_FEATURES[i % len(CURRENT_FEATURES)]] ** 2\n",
        "                    synthetic_features.append(feature_name)\n",
        "                else:\n",
        "                    # For any remaining features needed, use random values\n",
        "                    feature_name = f\"SYNTH_{i+1}\"\n",
        "                    X_prepared[feature_name] = np.random.rand(len(X_prepared))\n",
        "                    synthetic_features.append(feature_name)\n",
        "\n",
        "            st.write(f\"Added {features_to_add} synthetic features to match model requirements.\")\n",
        "\n",
        "            # Now X_prepared has the right number of features for prediction\n",
        "            X = X_prepared\n",
        "\n",
        "        else:\n",
        "            # Standard processing for the current features\n",
        "            X = df[CURRENT_FEATURES].copy()\n",
        "\n",
        "            # Fill NaNs with column means\n",
        "            for col in X.columns:\n",
        "                X[col] = X[col].fillna(X[col].mean())\n",
        "                # Replace remaining NaNs (if a column is all NaN) with 0\n",
        "                X[col] = X[col].fillna(0)\n",
        "\n",
        "            # Scale features to [0, 1] range\n",
        "            scaler = MinMaxScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            X = pd.DataFrame(X_scaled, columns=CURRENT_FEATURES)\n",
        "\n",
        "        st.write(\"Preprocessed Data Preview:\")\n",
        "        st.dataframe(X.head())\n",
        "\n",
        "        # Predict lithology labels\n",
        "        st.subheader(\"Step 5: Predicting Lithology Labels\")\n",
        "\n",
        "        try:\n",
        "            predictions = model.predict(X)\n",
        "            df['LITHOLOGY_LABEL'] = [LITHOLOGY_MAPPING.get(int(pred), f\"Unknown-{pred}\") for pred in predictions]\n",
        "            st.write(\"Data with Predicted Lithology Labels:\")\n",
        "            st.dataframe(df[['DEPT'] + CURRENT_FEATURES + ['LITHOLOGY_LABEL']].head())\n",
        "\n",
        "            # Create new LAS file with numeric predictions\n",
        "            st.subheader(\"Step 6: Generating Output LAS File\")\n",
        "            las_out = lasio.LASFile()\n",
        "\n",
        "            # Copy metadata from original LAS file\n",
        "            las_out.well = las.well\n",
        "            las_out.curves = las.curves\n",
        "            las_out.params = las.params\n",
        "            las_out.other = las.other\n",
        "\n",
        "            # Add depth and features to the new LAS file\n",
        "            las_out.append_curve('DEPT', df['DEPT'], unit='FT', descr='Depth')\n",
        "            for feature in CURRENT_FEATURES:\n",
        "                las_out.append_curve(feature, df[feature],\n",
        "                                    unit=las.curves[feature].unit if feature in las.curves else '',\n",
        "                                    descr=las.curves[feature].descr if feature in las.curves else '')\n",
        "\n",
        "            # Add predicted lithology as a numeric curve\n",
        "            las_out.append_curve('LITHOLOGY', predictions, unit='',\n",
        "                                descr='Predicted Lithology (0=Shale, 1=Dolomite, 2=Limestone, 3=Sandstone, 4=Siltstone)')\n",
        "\n",
        "            # Generate downloadable files\n",
        "            las_string = StringIO()\n",
        "            las_out.write(las_string)\n",
        "            las_string.seek(0)\n",
        "\n",
        "            # Provide download button for the LAS file\n",
        "            st.download_button(\n",
        "                label=\"Download LAS File with Numeric Lithology Codes\",\n",
        "                data=las_string.getvalue(),\n",
        "                file_name=\"output_with_lithology.las\",\n",
        "                mime=\"text/plain\"\n",
        "            )\n",
        "\n",
        "            # Provide a downloadable CSV with depth and lithology labels\n",
        "            st.subheader(\"Step 7: Download Lithology Labels as CSV\")\n",
        "            output_df = df[['DEPT', 'LITHOLOGY_LABEL']]\n",
        "            csv = output_df.to_csv(index=False)\n",
        "            st.download_button(\n",
        "                label=\"Download Lithology Labels (Depth and Names)\",\n",
        "                data=csv,\n",
        "                file_name=\"lithology_labels.csv\",\n",
        "                mime=\"text/csv\"\n",
        "            )\n",
        "\n",
        "            # Plotting\n",
        "            st.subheader(\"Step 8: Visualizing Well Logs and Lithology\")\n",
        "\n",
        "# Let user choose between standard plot and enhanced lithology plot\n",
        "plot_type = st.radio(\n",
        "    \"Select visualization type:\",\n",
        "    [\"Standard log display with lithology\", \"Enhanced lithology visualization\"]\n",
        ")\n",
        "\n",
        "if plot_type == \"Standard log display with lithology\":\n",
        "    # Select features to display\n",
        "    plot_features = st.multiselect(\"Select logs to display:\", CURRENT_FEATURES,\n",
        "                                  default=CURRENT_FEATURES[:min(4, len(CURRENT_FEATURES))])\n",
        "\n",
        "    if plot_features:\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=len(plot_features) + 1, figsize=(15, 10), sharey=True)\n",
        "\n",
        "        # Handle single feature case\n",
        "        if len(plot_features) == 1:\n",
        "            axes = [axes[0], axes[1]]\n",
        "\n",
        "        # Plot each feature\n",
        "        for i, feature in enumerate(plot_features):\n",
        "            axes[i].plot(df[feature], df['DEPT'], label=feature)\n",
        "            axes[i].set_title(feature)\n",
        "            axes[i].invert_yaxis()\n",
        "            axes[i].grid(True)\n",
        "            if i == 0:\n",
        "                axes[i].set_ylabel('Depth (ft)')\n",
        "\n",
        "        # Plot lithology\n",
        "        lith_colors = {'Shale': 'gray', 'Dolomite': 'blue', 'Limestone': 'green', 'Sandstone': 'yellow', 'Siltstone': 'red'}\n",
        "        lith_idx = len(plot_features)\n",
        "\n",
        "        # Create color list\n",
        "        colors = [lith_colors.get(label, 'black') for label in df['LITHOLOGY_LABEL']]\n",
        "\n",
        "        # Plot lithology with wider bars\n",
        "        bar_width = 0.8  # Make bars wider\n",
        "        for i, (depth, lithology) in enumerate(zip(df['DEPT'], df['LITHOLOGY_LABEL'])):\n",
        "            axes[lith_idx].barh(depth, bar_width, height=2, color=lith_colors.get(lithology, 'black'))\n",
        "\n",
        "        axes[lith_idx].set_title('Lithology')\n",
        "        axes[lith_idx].set_xticks([])\n",
        "\n",
        "        # Create legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [Patch(facecolor=color, label=name) for name, color in lith_colors.items()]\n",
        "        axes[lith_idx].legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        axes[lith_idx].invert_yaxis()\n",
        "        axes[lith_idx].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "else:\n",
        "    # Enhanced lithology visualization\n",
        "    # Define a function to create the enhanced lithology plot\n",
        "    def create_enhanced_lithology_plot(df, depth_col='DEPT', lithology_col='LITHOLOGY_LABEL'):\n",
        "        # Define a wider figure for better visualization\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 18),\n",
        "                                 gridspec_kw={'width_ratios': [1, 3]}, sharey=True)\n",
        "\n",
        "        # Define lithology colors and patterns\n",
        "        lith_colors = {\n",
        "            'Shale': ('gray', '//'),\n",
        "            'Dolomite': ('skyblue', 'o'),\n",
        "            'Limestone': ('green', '.'),\n",
        "            'Sandstone': ('yellow', ''),\n",
        "            'Siltstone': ('brown', '+')\n",
        "        }\n",
        "\n",
        "        # Plot depth scale on left axis\n",
        "        depth_range = df[depth_col].max() - df[depth_col].min()\n",
        "        axes[0].set_ylim(df[depth_col].max() + depth_range*0.02, df[depth_col].min() - depth_range*0.02)\n",
        "        axes[0].set_ylabel('Depth (ft)', fontsize=14)\n",
        "        axes[0].set_xticks([])\n",
        "        axes[0].grid(axis='y')\n",
        "\n",
        "        # Add depth labels at regular intervals\n",
        "        depth_interval = depth_range / 20\n",
        "        depth_interval = round(depth_interval / 50) * 50  # Round to nearest 50\n",
        "        depth_interval = max(50, depth_interval)  # Ensure minimum interval of 50\n",
        "\n",
        "        depth_min = df[depth_col].min()\n",
        "        depth_max = df[depth_col].max()\n",
        "\n",
        "        depth_ticks = np.arange(\n",
        "            depth_min - (depth_min % depth_interval),\n",
        "            depth_max + depth_interval,\n",
        "            depth_interval\n",
        "        )\n",
        "        axes[0].set_yticks(depth_ticks)\n",
        "\n",
        "        # Draw lithology column\n",
        "        # Group continuous lithology sections\n",
        "        current_lith = None\n",
        "        lith_sections = []\n",
        "        start_depth = None\n",
        "\n",
        "        for depth, lithology in zip(df[depth_col], df[lithology_col]):\n",
        "            if lithology != current_lith:\n",
        "                if current_lith is not None:\n",
        "                    lith_sections.append((start_depth, depth, current_lith))\n",
        "                current_lith = lithology\n",
        "                start_depth = depth\n",
        "\n",
        "        # Add the last section\n",
        "        if current_lith is not None and start_depth is not None:\n",
        "            lith_sections.append((start_depth, df[depth_col].iloc[-1], current_lith))\n",
        "\n",
        "        # Plot each lithology section as a rectangle\n",
        "        for start, end, lithology in lith_sections:\n",
        "            if lithology in lith_colors:\n",
        "                color, hatch = lith_colors[lithology]\n",
        "                # Draw rectangle for this lithology section\n",
        "                rect = plt.Rectangle((0, end), 1, start - end,\n",
        "                                    facecolor=color,\n",
        "                                    hatch=hatch,\n",
        "                                    edgecolor='black',\n",
        "                                    lw=0.5,\n",
        "                                    alpha=0.8)\n",
        "                axes[1].add_patch(rect)\n",
        "\n",
        "        # Set up the lithology axis\n",
        "        axes[1].set_xlim(0, 1)\n",
        "        axes[1].set_xticks([0.5])\n",
        "        axes[1].set_xticklabels(['Lithology'], fontsize=14)\n",
        "        axes[1].tick_params(axis='x', direction='out', rotation=45)\n",
        "        axes[1].grid(False)\n",
        "\n",
        "        # Create legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor=color, hatch=hatch, edgecolor='black', label=name)\n",
        "            for name, (color, hatch) in lith_colors.items()\n",
        "        ]\n",
        "        axes[1].legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
        "\n",
        "        # Add title\n",
        "        plt.suptitle('Lithology Log', fontsize=16, y=0.98)\n",
        "\n",
        "        # Add depth markers\n",
        "        for i, depth in enumerate(depth_ticks):\n",
        "            if depth >= df[depth_col].min() and depth <= df[depth_col].max():\n",
        "                axes[0].axhline(y=depth, color='black', linestyle='-', alpha=0.3, lw=0.5)\n",
        "                axes[1].axhline(y=depth, color='black', linestyle='-', alpha=0.3, lw=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        fig.subplots_adjust(wspace=0.05)\n",
        "        return fig\n",
        "\n",
        "    # Create and display the enhanced lithology plot\n",
        "    fig = create_enhanced_lithology_plot(df)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Optional: Display logs alongside lithology in a separate plot\n",
        "    show_logs = st.checkbox(\"Show well logs alongside lithology\")\n",
        "\n",
        "    if show_logs:\n",
        "        # Select features to display\n",
        "        plot_features = st.multiselect(\"Select logs to display:\", CURRENT_FEATURES,\n",
        "                                      default=CURRENT_FEATURES[:min(3, len(CURRENT_FEATURES))])\n",
        "\n",
        "        if plot_features:\n",
        "            # Create a multi-panel figure with logs and lithology\n",
        "            fig2, axes = plt.subplots(nrows=1, ncols=len(plot_features) + 1,\n",
        "                                     figsize=(16, 12), sharey=True)\n",
        "\n",
        "            # Handle single feature case\n",
        "            if len(plot_features) == 1:\n",
        "                axes = [axes[0], axes[1]]\n",
        "\n",
        "            # Plot each feature\n",
        "            for i, feature in enumerate(plot_features):\n",
        "                axes[i].plot(df[feature], df['DEPT'], label=feature)\n",
        "                axes[i].set_title(feature)\n",
        "                axes[i].invert_yaxis()\n",
        "                axes[i].grid(True)\n",
        "                if i == 0:\n",
        "                    axes[i].set_ylabel('Depth (ft)')\n",
        "\n",
        "            # Plot lithology as color-coded column\n",
        "            lith_idx = len(plot_features)\n",
        "            lith_colors = {\n",
        "                'Shale': 'gray',\n",
        "                'Dolomite': 'skyblue',\n",
        "                'Limestone': 'green',\n",
        "                'Sandstone': 'yellow',\n",
        "                'Siltstone': 'brown'\n",
        "            }\n",
        "\n",
        "            # Group continuous lithology sections\n",
        "            current_lith = None\n",
        "            lith_sections = []\n",
        "            start_depth = None\n",
        "\n",
        "            for depth, lithology in zip(df['DEPT'], df['LITHOLOGY_LABEL']):\n",
        "                if lithology != current_lith:\n",
        "                    if current_lith is not None:\n",
        "                        lith_sections.append((start_depth, depth, current_lith))\n",
        "                    current_lith = lithology\n",
        "                    start_depth = depth\n",
        "\n",
        "            # Add the last section\n",
        "            if current_lith is not None and start_depth is not None:\n",
        "                lith_sections.append((start_depth, df['DEPT'].iloc[-1], current_lith))\n",
        "\n",
        "            # Plot each lithology section as a rectangle\n",
        "            for start, end, lithology in lith_sections:\n",
        "                if lithology in lith_colors:\n",
        "                    # Draw rectangle for this lithology section\n",
        "                    rect = plt.Rectangle((0, end), 1, start - end,\n",
        "                                        facecolor=lith_colors[lithology],\n",
        "                                        edgecolor='black',\n",
        "                                        lw=0.5)\n",
        "                    axes[lith_idx].add_patch(rect)\n",
        "\n",
        "            # Set up the lithology axis\n",
        "            axes[lith_idx].set_xlim(0, 1)\n",
        "            axes[lith_idx].set_xticks([])\n",
        "            axes[lith_idx].set_title('Lithology')\n",
        "\n",
        "            # Create legend\n",
        "            from matplotlib.patches import Patch\n",
        "            legend_elements = [\n",
        "                Patch(facecolor=color, edgecolor='black', label=name)\n",
        "                for name, color in lith_colors.items()\n",
        "            ]\n",
        "            axes[lith_idx].legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig2)\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload an LAS file to begin.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVh8IxI2RBhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}