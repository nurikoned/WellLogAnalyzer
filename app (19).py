# -*- coding: utf-8 -*-
"""App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CehlEwPz1nguAsyS1yBoLCC57midBC-l
"""

# pip install streamlit lasio pandas numpy scikit-learn xgboost matplotlib

import joblib
from xgboost import XGBClassifier # or XGBRegressor, depending on your task

model = joblib.load('model.pkl')

joblib.dump(model, 'model.pkl')

import streamlit as st
import lasio
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib
import matplotlib.pyplot as plt
from io import StringIO, BytesIO, TextIOWrapper

# Define the features the model was trained on
TRAINED_FEATURES = ['GR', 'RHOB','RILD', 'DT', 'SP', 'SPOR', 'RILM', 'RLL3']
LITHOLOGY_MAPPING = {0: 'Shale', 1: 'Dolomite', 2: 'Limestone', 3: 'Sandstone', 4: 'Siltstone'}

# Load the pre-trained model
try:
    model = joblib.load('model.pkl')
except FileNotFoundError:
    st.error("Model file 'model.pkl' not found. Please ensure the model file is in the same directory as this script.")
    st.stop()

# Streamlit app setup
st.title("Kansas Basin Well Log Analyzer")
st.markdown("Upload an LAS file to preprocess, predict lithology labels, and visualize the results.")

# File uploader for LAS file
uploaded_file = st.file_uploader("Upload an LAS file", type=['las'])

if uploaded_file is not None:
    # Read the LAS file
    st.subheader("Step 1: Reading the LAS File")
    try:
        # Read the file content as bytes and wrap in TextIOWrapper for lasio
        file_content = uploaded_file.read()
        text_stream = TextIOWrapper(BytesIO(file_content), encoding='utf-8')
        las = lasio.read(text_stream)
        df = las.df().reset_index()  # Convert LAS to DataFrame
        st.write("Original Data Preview:")
        st.dataframe(df.head())
    except Exception as e:
        st.error(f"Error reading LAS file: {e}")
        st.stop()

    # Check if 'DEPTH' column exists and rename to 'DEPT'
    if 'DEPTH' in df.columns:
        st.write("Renaming 'DEPTH' column to 'DEPT' for consistency...")
        df.rename(columns={'DEPTH': 'DEPT'}, inplace=True)
        # Update the LAS object to reflect the renamed column
        for curve in las.curves:
            if curve.mnemonic == 'DEPTH':
                curve.mnemonic = 'DEPT'
                break
    elif 'DEPT' not in df.columns:
        st.error("No 'DEPT' or 'DEPTH' column found in the LAS file. Please ensure the file contains a depth column.")
        st.stop()

    # Add missing features as NaN columns
    missing_features = [f for f in TRAINED_FEATURES if f not in df.columns]
    for feature in missing_features:
        df[feature] = np.nan  # Add missing columns with NaN

    # Remove rows where all features are NaN
    st.subheader("Step 1.5: Filtering Rows with Missing Features")
    st.write("Removing rows where all features (GR, RHOB, RILD, DT, SP, SPOR) are NaN...")
    # Debug: Show the state of TRAINED_FEATURES columns before filtering
    st.write("Feature Columns (Before Filtering):")
    st.dataframe(df[TRAINED_FEATURES].head())

    # Filter out rows where all TRAINED_FEATURES are NaN
    df_filtered = df.dropna(subset=TRAINED_FEATURES, how='all')

    # Debug: Show the filtered DataFrame
    st.write("Data After Filtering (Rows with at least one non-NaN feature):")
    st.dataframe(df_filtered.head())

    # Check if the filtered DataFrame is empty
    if df_filtered.empty:
        st.error("After filtering, no rows remain because all features are NaN in all rows. Please upload an LAS file with at least some non-NaN feature data.")
        st.stop()

    # Update df to use the filtered DataFrame
    df = df_filtered

    # Reorder columns to match training order
    df = df[['DEPT'] + TRAINED_FEATURES]  # Ensure 'DEPT' is the depth column

    # Warn user if some features are missing
    if missing_features:
        st.warning(f"Features {missing_features} are missing and were added as NaN. Predictions may be less accurate.")

    # Preprocess the data
    st.subheader("Step 2: Preprocessing the Data")
    # Scale features to [0, 1] range
    st.write("Scaling features to [0, 1] range...")
    scaler = MinMaxScaler()
    df[TRAINED_FEATURES] = scaler.fit_transform(df[TRAINED_FEATURES])
    st.write("Preprocessed Data Preview:")
    st.dataframe(df.head())

    # Predict lithology labels
    st.subheader("Step 3: Predicting Lithology Labels")
    X = df[TRAINED_FEATURES]
    predictions = model.predict(X)
    df['LITHOLOGY_LABEL'] = [LITHOLOGY_MAPPING[pred] for pred in predictions]
    st.write("Data with Predicted Lithology Labels:")
    st.dataframe(df[['DEPT'] + TRAINED_FEATURES + ['LITHOLOGY_LABEL']].head())

    # Create new LAS file with numeric predictions
    st.subheader("Step 4: Generating Output LAS File")
    las_out = lasio.LASFile()

    # Copy metadata from original LAS file
    las_out.well = las.well
    las_out.curves = las.curves
    las_out.params = las.params
    las_out.other = las.other

    # Add depth and features to the new LAS file
    las_out.append_curve('DEPT', df['DEPT'], unit='FT', descr='Depth')
    for feature in TRAINED_FEATURES:
        las_out.append_curve(feature, df[feature], unit=las.curves[feature].unit if feature in las.curves else '', descr=las.curves[feature].descr if feature in las.curves else '')

    # Add predicted lithology as a numeric curve
    las_out.append_curve('LITHOLOGY', predictions, unit='', descr='Predicted Lithology (0=Shale, 1=Dolomite, 2=Limestone, 3=Sandstone, 4=Siltstone)')

    # Add a custom section with depth-to-label mapping (for reference only)
    lithology_text = "\n".join([f"Depth {d:.2f}: {label}" for d, label in zip(df['DEPT'], df['LITHOLOGY_LABEL'])])
    las_out.other = f"Lithology Labels by Depth:\n{lithology_text}\n\nOriginal Other Section:\n{las_out.other}"

    # Convert LAS to string for download
    las_string = StringIO()
    las_out.write(las_string)
    las_string.seek(0)

    # Provide download button for the LAS file
    st.download_button(
        label="Download LAS File with Numeric Lithology Codes",
        data=las_string.getvalue(),
        file_name="output_with_lithology.las",
        mime="text/plain"
    )

    # Provide a downloadable CSV with depth and lithology labels
    st.subheader("Step 5: Download Lithology Labels as CSV")
    output_df = df[['DEPT', 'LITHOLOGY_LABEL']]
    csv = output_df.to_csv(index=False)
    st.download_button(
        label="Download Lithology Labels (Depth and Names)",
        data=csv,
        file_name="lithology_labels.csv",
        mime="text/csv"
    )

    # Plotting
    st.subheader("Step 6: Visualizing Well Logs and Lithology")
    fig, axes = plt.subplots(nrows=1, ncols=len(TRAINED_FEATURES) + 1, figsize=(15, 10), sharey=True)

    # Plot each feature
    for i, feature in enumerate(TRAINED_FEATURES):
        axes[i].plot(df[feature], df['DEPT'], label=feature)
        axes[i].set_title(feature)
        axes[i].invert_yaxis()
        axes[i].grid(True)
        if i == 0:
            axes[i].set_ylabel('Depth (ft)')

    # Plot lithology
    lith_colors = {'Shale': 'gray', 'Dolomite': 'blue', 'Limestone': 'green', 'Sandstone': 'yellow', 'Siltstone': 'red'}
    lith_numeric = df['LITHOLOGY_LABEL'].map({v: k for k, v in LITHOLOGY_MAPPING.items()})
    axes[-1].scatter(lith_numeric, df['DEPT'], c=df['LITHOLOGY_LABEL'].map(lith_colors), label='Lithology')
    axes[-1].set_title('Lithology')
    axes[-1].set_xticks(range(len(LITHOLOGY_MAPPING)))
    axes[-1].set_xticklabels(LITHOLOGY_MAPPING.values(), rotation=45)
    axes[-1].invert_yaxis()
    axes[-1].grid(True)

    plt.tight_layout()
    st.pyplot(fig)

else:
    st.info("Please upload an LAS file to begin.")

